{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "view-in-github"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/Peace3B/Math_Tutor_LLM_Summative_Assignment/blob/main/Math_Tutor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "klvq_iEqnSYP"
            },
            "source": [
                " **Math Education Assistant: Fine-tuning LLM with LoRA**\n",
                "\n",
                "Project Overview This notebook demonstrates fine-tuning a Large Language Model for math education using;\n",
                "\n",
                "Dataset: UltraData-Math from Hugging Face\n",
                "\n",
                "Base Model: TinyLlama-1.1B\n",
                "Method: LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning\n",
                "Domain: Mathematics Education\n",
                "\n",
                "*Table of Contents*\n",
                "  1. Setup and Installation\n",
                "  2. Data Loading and Exploration\n",
                "  3. Data Preprocessing\n",
                "  4. Model Selection and LoRA Configuration\n",
                "  5. Training with Hyperparameter Experiments\n",
                "  6. Evaluation Metrics\n",
                "  7. Model Comparison (Base vs Fine-tuned)\n",
                "  8. Deployment with Gradio"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HEsbRlEwLpi6"
            },
            "source": [
                "**1. Setup and Installation**\n",
                "\n",
                "Install required libraries for fine-tuning with LoRA and deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ZpR92kanoV5g",
                "outputId": "cbc3d680-0b09-4b21-9a9a-313b131883ad"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/bin/bash: -c: line 1: unexpected EOF while looking for matching `\"'\n",
                        "/bin/bash: -c: line 2: syntax error: unexpected end of file\n",
                        "/bin/bash: -c: line 1: unexpected EOF while looking for matching `\"'\n",
                        "/bin/bash: -c: line 2: syntax error: unexpected end of file\n"
                    ]
                }
            ],
            "source": [
                "!pip install -q transformers datasets peft accelerate bitsandbytes gradio evaluate rouge-score nltk torch pandas tqdm\\n\",\n",
                "!pip install -q sentencepiece protobuf sacrebleu\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "NM76uFHHqawT",
                "outputId": "5570ac9b-bc9a-4d8f-a12a-237be63d9938"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
                        "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
                        "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
                        "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
                        "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
                        "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.3)\n",
                        "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
                        "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
                        "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
                        "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (1.4.1)\n",
                        "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (26.0)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.24.2)\n",
                        "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
                        "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
                        "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (0.28.1)\n",
                        "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.5.4)\n",
                        "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (0.24.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2026.1.4)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
                        "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.1)\n",
                        "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
                        "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (4.12.1)\n",
                        "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (1.0.9)\n",
                        "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (0.16.0)\n",
                        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
                        "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.7.0->evaluate) (0.24.0)\n",
                        "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (8.3.1)\n",
                        "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (13.9.4)\n",
                        "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (0.0.4)\n",
                        "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (4.0.0)\n",
                        "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (2.19.2)\n",
                        "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (0.1.2)\n"
                    ]
                }
            ],
            "source": [
                "!pip install evaluate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "KtIohJxKou-6",
                "outputId": "81c14646-3f63-4917-c905-7f1c100d3721"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n",
                        "GPU: Tesla T4\n",
                        "GPU Memory: 15.64 GB\n"
                    ]
                }
            ],
            "source": [
                "# Import libraries\n",
                "import os\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "from tqdm import tqdm\n",
                "import evaluate\n",
                "import gradio as gr\n",
                "\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForCausalLM,\n",
                "    TrainingArguments,\n",
                "    Trainer\n",
                ")\n",
                "\n",
                "from peft import (\n",
                "    LoraConfig,\n",
                "    get_peft_model,\n",
                "    TaskType,\n",
                "    prepare_model_for_kbit_training\n",
                ")\n",
                "\n",
                "# Suppress warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "# Check GPU availability\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(\n",
                "        f\"GPU Memory: \"\n",
                "        f\"{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HHbIox_-L2rC"
            },
            "source": [
                "**2. Data Loading and Exploration**\n",
                "\n",
                "Load the UltraData-Math dataset and explore its structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 327,
                    "referenced_widgets": [
                        "310b1e091c1547e3b3506ac4563406e7",
                        "ab587f69d3b64e3b9f776c267165a4c1",
                        "e45f9e9067e34b9999fe43095137aeab",
                        "94f6675a5e9243f68508292875e2c8e1",
                        "12a1308f534b489687d03fee3153ee9a",
                        "2cee10c90144439a863cd302e90495bd",
                        "0ce4f4abac8f4aac8fdd92d09261a495",
                        "efffc24cc6de4f89bffe4d965452c9e0",
                        "66a6dcbf75154ab2b9b700d4b41da020",
                        "c0c909aa6a90497984c16046574de763",
                        "1660d05c5b734f0fa49bdf792fbc759a",
                        "280d692a05174e09a32e5c6f83199e13",
                        "a3a2da016f6744d29f7ceccd5f8a9034",
                        "6228ea3cf0fe4126ad3ec4da606f1369",
                        "5cbaa3ef4f7d4490b0f3cbadfdd32419",
                        "4048b1a95ead440da8d487479ec56371",
                        "2086b7f26eb8416982f52750aed61eb0",
                        "96abf9b404224b8abc9fa7f6ad94e55a",
                        "da56299852c148a0b34b260b9020f76f",
                        "3b19ffa6938e42819a50a318f902cdf9",
                        "25651a1499ea414fb436f996036c6c09",
                        "55037ba8d50c49698eb7e8dba514b2d6"
                    ]
                },
                "id": "7rYwVIg8rWD9",
                "outputId": "aa183ec8-b63e-47f8-b2cc-040e746e570c"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading UltraData-Math dataset in streaming mode...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
                        "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "310b1e091c1547e3b3506ac4563406e7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "280d692a05174e09a32e5c6f83199e13",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Dataset features:\n",
                        "{'uid': Value('string'), 'content': Value('string')}\n",
                        "\n",
                        "First 5 examples (from streaming data):\n",
                        "{'uid': '6f55f96b-a0eb-4b93-9af2-68367ae2e744', 'content': \"Student: I'm having trouble understanding the problem statement for the integral \\\\(\\\\int_{0}^{2\\\\pi}(x-\\\\pi)^2 (\\\\sin x) dx\\\\). Could you explain it to me step-by-step?\\n\\nTeacher: Of course! Let's break it down. The problem is to evaluate the integral \\\\(\\\\int_{0}^{2\\\\pi}(x-\\\\pi)^2 (\\\\sin x) dx\\\\).\\n\\nStudent: Okay, I see the integral. But why do we need to use substitution here?\\n\\nTeacher: Good question. Using substitution can simplify the integral. We let \\\\( t = x - \\\\pi \\\\). This substitution helps us center the integral around zero, which can make it easier to apply symmetry properties.\\n\\nStudent: I see. So, what happens to the limits of integration when we make this substitution?\\n\\nTeacher: When \\\\( x = 0 \\\\), \\\\( t = -\\\\pi \\\\). When \\\\( x = 2\\\\pi \\\\), \\\\( t = \\\\pi \\\\). So, the new limits of integration are from \\\\(-\\\\pi\\\\) to \\\\(\\\\pi\\\\).\\n\\nStudent: Got it. And what about the differential \\\\( dx \\\\)?\\n\\nTeacher: Since \\\\( t = x - \\\\pi \\\\), we have \\\\( \\\\frac{d}{dx}(x-\\\\pi) = dt \\\\), which means \\\\( dx = dt \\\\).\\n\\nStudent: That makes sense. So, how does the integral transform after the substitution?\\n\\nTeacher: The integral \\\\(\\\\int_{0}^{2\\\\pi} (x-\\\\pi)^2 \\\\sin x \\\\, dx\\\\) becomes \\\\(\\\\int_{-\\\\pi}^{\\\\pi} t^2 \\\\sin(t+\\\\pi) \\\\, dt\\\\).\\n\\nStudent: I'm a bit confused about \\\\(\\\\sin(t+\\\\pi)\\\\). Can you explain that part?\\n\\nTeacher: Sure. We know that \\\\(\\\\sin(t+\\\\pi) = -\\\\sin t\\\\). This is a trigonometric identity. So, the integral becomes \\\\(\\\\int_{-\\\\pi}^{\\\\pi} t^2 (-\\\\sin t) \\\\, dt\\\\), which simplifies to \\\\(-\\\\int_{-\\\\pi}^{\\\\pi} t^2 \\\\sin t \\\\, dt\\\\).\\n\\nStudent: Okay, I understand that. But why does the integral evaluate to zero?\\n\\nTeacher: The key here is to recognize that \\\\( t^2 \\\\) is an even function and \\\\(\\\\sin t\\\\) is an odd function. The product \\\\( t^2 \\\\sin t \\\\) is an odd function. The integral of an odd function over symmetric limits \\\\([-a, a]\\\\) is zero.\\n\\nStudent: Ah, I see. So, the integral \\\\(\\\\int_{-\\\\pi}^{\\\\pi} t^2 \\\\sin t \\\\, dt\\\\) is zero because \\\\( t^2 \\\\sin t \\\\) is an odd function?\\n\\nTeacher: Exactly. The integral of an odd function over symmetric limits is zero. Therefore, the value of the integral \\\\(\\\\int_{0}^{2\\\\pi}(x-\\\\pi)^2 (\\\\sin x) dx\\\\) is \\\\(0\\\\).\\n\\nStudent: Thank you, that clears up a lot of my confusion. So, the correct answer is \\\\(0\\\\).\\n\\nTeacher: Yes, the correct answer is \\\\(0\\\\). I'm glad I could help you understand the solution.\"}\n",
                        "{'uid': '447d5575-52fd-47f7-b80a-7c781c5d8797', 'content': \"User: Can you help me understand how to find the equation of a line that is parallel to \\\\(y = 5x + 12\\\\) and passes through the point \\\\((-2, -4)\\\\)?\\nQwen: Sure, to find the equation of a line parallel to \\\\(y = 5x + 12\\\\) and passing through the point \\\\((-2, -4)\\\\), we need to remember that parallel lines have the same slope. The slope of the given line is 5. We can use the point-slope form of the equation of a line, which is \\\\(y - y_1 = m(x - x_1)\\\\), where \\\\(m\\\\) is the slope and \\\\((x_1, y_1)\\\\) is the point the line passes through. Substituting \\\\(m = 5\\\\), \\\\(x_1 = -2\\\\), and \\\\(y_1 = -4\\\\), we get:\\n\\\\[ y + 4 = 5(x + 2) \\\\]\\nUser: What's the next step to simplify this equation?\\nQwen: The next step is to simplify the equation. Distribute the 5 on the right side:\\n\\\\[ y + 4 = 5x + 10 \\\\]\\nThen, isolate \\\\(y\\\\) by subtracting 4 from both sides:\\n\\\\[ y = 5x + 6 \\\\]\\nSo, the equation of the line is \\\\(y = 5x + 6\\\\).\\n\\nUser: I see, thanks! How about finding a pair of numbers from the list 17, 24, 25, 26, 35, 43, 44 that have a difference of 20?\\nQwen: To find a pair of numbers with a difference of 20 from the list, we can subtract each number from the others and see which pair gives us 20. Let's check:\\n\\\\[ 44 - 24 = 20 \\\\]\\nSo, the pair of numbers that have a difference of 20 is 24 and 44.\\n\\nUser: That makes sense. Now, if \\\\(x = \\\\sqrt{7x - 1}\\\\), how do I find the value of \\\\(x^2 + \\\\frac{1}{x^2}\\\\)?\\nQwen: First, we need to solve for \\\\(x\\\\) from the equation \\\\(x = \\\\sqrt{7x - 1}\\\\). Square both sides to eliminate the square root:\\n\\\\[ x^2 = 7x - 1 \\\\]\\nRearrange the equation to form a standard quadratic equation:\\n\\\\[ x^2 - 7x + 1 = 0 \\\\]\\nWe can solve this quadratic equation using the quadratic formula \\\\(x = \\\\frac{-b \\\\pm \\\\sqrt{b^2 - 4ac}}{2a}\\\\), where \\\\(a = 1\\\\), \\\\(b = -7\\\\), and \\\\(c = 1\\\\):\\n\\\\[ x = \\\\frac{7 \\\\pm \\\\sqrt{49 - 4}}{2} \\\\]\\n\\\\[ x = \\\\frac{7 \\\\pm \\\\sqrt{45}}{2} \\\\]\\n\\\\[ x = \\\\frac{7 \\\\pm 3\\\\sqrt{5}}{2} \\\\]\\nWe choose the positive root for \\\\(x\\\\):\\n\\\\[ x = \\\\frac{7 + 3\\\\sqrt{5}}{2} \\\\]\\nTo find \\\\(x^2 + \\\\frac{1}{x^2}\\\\), we use the identity:\\n\\\\[ x^2 + \\\\frac{1}{x^2} = (x + \\\\frac{1}{x})^2 - 2 \\\\]\\nFirst, find \\\\(x + \\\\frac{1}{x}\\\\):\\n\\\\[ x + \\\\frac{1}{x} = \\\\frac{7 + 3\\\\sqrt{5}}{2} + \\\\frac{2}{7 + 3\\\\sqrt{5}} \\\\]\\nRationalize the second term:\\n\\\\[ \\\\frac{2}{7 + 3\\\\sqrt{5}} \\\\cdot \\\\frac{7 - 3\\\\sqrt{5}}{7 - 3\\\\sqrt{5}} = \\\\frac{14 - 6\\\\sqrt{5}}{49 - 45} = \\\\frac{14 - 6\\\\sqrt{5}}{4} = \\\\frac{7 - 3\\\\sqrt{5}}{2} \\\\]\\nSo:\\n\\\\[ x + \\\\frac{1}{x} = \\\\frac{7 + 3\\\\sqrt{5}}{2} + \\\\frac{7 - 3\\\\sqrt{5}}{2} = 7 \\\\]\\nNow:\\n\\\\[ x^2 + \\\\frac{1}{x^2} = 7^2 - 2 = 49 - 2 = 47 \\\\]\\nThus, the value of \\\\(x^2 + \\\\frac{1}{x^2}\\\\) is 47.\\n\\nUser: Got it, thanks! How do I solve the equation \\\\(\\\\frac{n}{15} \\\\times 3 - 9.5 = 45.5\\\\)?\\nQwen: Let's solve the equation step by step. First, simplify the left side:\\n\\\\[ \\\\frac{n}{15} \\\\times 3 - 9.5 = 45.5 \\\\]\\n\\\\[ \\\\frac{n}{5} - 9.5 = 45.5 \\\\]\\nNext, add 9.5 to both sides to isolate the term with \\\\(n\\\\):\\n\\\\[ \\\\frac{n}{5} = 55 \\\\]\\nFinally, multiply both sides by 5 to solve for \\\\(n\\\\):\\n\\\\[ n = 275 \\\\]\\nSo, the value of \\\\(n\\\\) is 275.\\n\\nUser: That's clear. How do I make \\\\(x\\\\) the subject in the equation \\\\(y = 4\\\\sqrt{x} + 3 - 7\\\\)?\\nQwen: First, simplify the equation:\\n\\\\[ y = 4\\\\sqrt{x} - 4 \\\\]\\nNext, add 4 to both sides to isolate the term with the square root:\\n\\\\[ y + 4 = 4\\\\sqrt{x} \\\\]\\nThen, divide both sides by 4 to solve for \\\\(\\\\sqrt{x}\\\\):\\n\\\\[ \\\\frac{y + 4}{4} = \\\\sqrt{x} \\\\]\\nFinally, square both sides to solve for \\\\(x\\\\):\\n\\\\[ x = \\\\left(\\\\frac{y + 4}{4}\\\\right)^2 \\\\]\\nSo, the equation with \\\\(x\\\\) as the subject is:\\n\\\\[ x = \\\\left(\\\\frac{y + 4}{4}\\\\right)^2 \\\\]\\n\\nUser: I understand. If my water pump can move 50 liters of water every minute and I left it on for 5 hours, how much water was pumped into the house?\\nQwen: First, convert the time from hours to minutes:\\n\\\\[ 5 \\\\text{ hours} = 5 \\\\times 60 = 300 \\\\text{ minutes} \\\\]\\nThen, calculate the total amount of water pumped:\\n\\\\[ 50 \\\\text{ liters/minute} \\\\times 300 \\\\text{ minutes} = 15000 \\\\text{ liters} \\\\]\\nSo, the water pump moved 15000 liters of water into the house.\\n\\nUser: Thanks! If a rectangle has an area of 36 cm\\\\(^2\\\\) and is four times as long as it is wide, what is its width?\\nQwen: Let the width of the rectangle be \\\\(w\\\\). Then the length is \\\\(4w\\\\). The area of the rectangle is given by:\\n\\\\[ w \\\\times 4w = 36 \\\\]\\n\\\\[ 4w^2 = 36 \\\\]\\nDivide both sides by 4:\\n\\\\[ w^2 = 9 \\\\]\\nTake the square root of both sides:\\n\\\\[ w = 3 \\\\]\\nSo, the width of the rectangle is 3 cm.\\n\\nUser: I see. Rachel spends 14% of her pocket money on chocolates and 18% on pizza. How much money will be left with her?\\nQwen: Let Rachel's total pocket money be \\\\(x\\\\). The amount spent on chocolates and pizza is:\\n\\\\[ 0.14x + 0.18x = 0.32x \\\\]\\nThe amount left with Rachel is:\\n\\\\[ x - 0.32x = 0.68x \\\\]\\nSo, Rachel will have 68% of her pocket money left.\\n\\nUser: If the number of ways of arranging the 26 letters of the English alphabet is \\\\(26!\\\\), and a computer prints these arrangements at the rate of 1000 per second, how long will this take?\\nQwen: First, calculate \\\\(26!\\\\):\\n\\\\[ 26! \\\\approx 4.0329146 \\\\times 10^{26} \\\\]\\nThe time in seconds to print all arrangements is:\\n\\\\[ \\\\frac{4.0329146 \\\\times 10^{26}}{1000} = 4.0329146 \\\\times 10^{23} \\\\]\\nConvert this time to years:\\n\\\\[ \\\\frac{4.0329146 \\\\times 10^{23}}{60 \\\\times 60 \\\\times 24 \\\\times 365.25} \\\\approx 1.279 \\\\times 10^{16} \\\\text{ years} \\\\]\\nSo, it will take approximately \\\\(1.279 \\\\times 10^{16}\\\\) years.\\n\\nUser: Fred is 25 years older than Bill and in 5 years he will be twice as old as Bill. What are their present ages?\\nQwen: Let Bill's age be \\\\(b\\\\). Then Fred's age is \\\\(b + 25\\\\). In 5 years, Fred's age will be \\\\(b + 25 + 5\\\\) and Bill's age will be \\\\(b + 5\\\\). According to the problem, in 5 years, Fred will be twice as old as Bill:\\n\\\\[ b + 30 = 2(b + 5) \\\\]\\nSimplify and solve for \\\\(b\\\\):\\n\\\\[ b + 30 = 2b + 10 \\\\]\\n\\\\[ 20 = b \\\\]\\nSo, Bill is 20 years old. Fred's age is:\\n\\\\[ b + 25 = 20 + 25 = 45 \\\\]\\nTherefore, Bill is 20 years old and Fred is 45 years old.\\n\\nUser: A farmer has 72 animals, goats and chickens together they have 168 legs. How many are goats?\\nQwen: Let \\\\(g\\\\) be the number of goats and \\\\(c\\\\) be the number of chickens. We have two equations:\\n\\\\[ g + c = 72 \\\\]\\n\\\\[ 4g + 2c = 168 \\\\]\\nFrom the first equation, solve for \\\\(c\\\\):\\n\\\\[ c = 72 - g \\\\]\\nSubstitute this into the second equation:\\n\\\\[ 4g + 2(72 - g) = 168 \\\\]\\n\\\\[ 4g + 144 - 2g = 168 \\\\]\\n\\\\[ 2g = 24 \\\\]\\n\\\\[ g = 12 \\\\]\\nSo, there are 12 goats.\\n\\nUser: In 1990, the populations of town A and town B were the same. From 1990 to 2000, the population of town A increased by 60% while the population of town B decreased by 20%. If town B's population was 24,000 in 2000, what was the population of town A in 2000?\\nQwen: Let the initial population of both towns be \\\\(p\\\\). The population of town B in 2000 is given by:\\n\\\\[ 0.8p = 24000 \\\\]\\nSolve for \\\\(p\\\\):\\n\\\\[ p = \\\\frac{24000}{0.8} = 30000 \\\\]\\nThe population of town A in 2000 is:\\n\\\\[ 1.6p = 1.6 \\\\times 30000 = 48000 \\\\]\\nSo, the population of town A in 2000 was 48,000.\"}\n",
                        "{'uid': 'ad695b53-7b8d-49b7-8a23-aaf724bebcc7', 'content': 'Layman: I\\'m trying to understand this reduction from Near-Clique to Clique problem. The document says that if I have a Near-Clique problem instance, I can construct a new Clique problem instance from it. But my intuition is that Clique is the \"harder\" problem, so wouldn\\'t you reduce the other way around, from Clique to Near-Clique? Am I thinking about this backwards?\\n\\nExpert: You are absolutely right to question that. Your intuition is correct in the context of proving NP-completeness. If our goal is to show that Near-Clique is NP-hard, we would need a reduction from a known NP-complete problem like Clique to Near-Clique. The reduction described here, from Near-Clique to Clique, is useful for a different purpose, perhaps classifying the complexity relationship between the two problems. For our discussion, we will stick to the provided reduction as it is.\\n\\nL: Okay, so let\\'s look at the construction itself. It says: \"For each edge pair of vertices `x,y \\\\in (N(u)\\\\cap N(v))`, if `xy \\\\in E(G)`, add the edge `x\\'y\\'` to `E(G\\')`.\" Could you unpack this for me? What are `x\\'` and `y\\'`?\\n\\nE: Let\\'s break it down. We start with the original graph `G`. The goal is to build `G\\'`. For every pair of vertices `(u,v)` in `G` that do not have an edge between them (i.e., `uv \\\\notin E(G)`), we are going to create a small, separate subgraph inside `G\\'`.\\n\\nThe vertices of this new subgraph will be called `x\\'`, where `x` is a vertex that is a common neighbor of both `u` and `v` in `G`. In other words, `x` is a vertex that shares an edge with `u` (`ux \\\\in E(G)`) and an edge with `v` (`vx \\\\in E(G)`). We use the notation `N(u) \\\\cap N(v)` to represent the set of all such common neighbors. So, for every vertex `x` in this set `N(u) \\\\cap N(v)`, we add a corresponding vertex `x\\'` to our new graph `G\\'`.\\n\\nNow, we need to connect these new vertices `x\\'`.\\n\\nE: To connect the new vertices, we follow the adjacency rules from the old graph. If two common neighbors `x` and `y` were connected by an edge in the original graph `G` (meaning `xy \\\\in E(G)`), then we add an edge between their corresponding copies `x\\'` and `y\\'` in the new graph `G\\'`.\\n\\nEssentially, each such subgraph in `G\\'` is an induced copy of the original graph `G`\\'s subgraph formed by the common neighbors of `u` and `v`. We do this for every non-edge `(u,v)` in the original graph `G`.\\n\\nL: That makes sense. So `G\\'` gets built by gluing together many of these small, induced subgraph \"patches,\" one for each missing edge in `G`.\\n\\nE: Exactly.\\n\\nL: And then it says to set `k\\' = k-2`. Why subtract two?\\n\\nE: The idea is that any large near-clique `S` in `G` of size `|S|=k` must be missing exactly one edge to be a full clique, say the edge `(u,v)`. This means that the `k-2` other vertices in the set (i.e., `S \\\\setminus \\\\{u,v\\\\}`) must form a complete subgraph among themselves, and each of them must be connected to both `u` and `v`.\\n\\nWhen we build `G\\'`, the vertices of this `k-2` sized clique are represented by the vertices `x\\'` within the specific subgraph corresponding to the non-edge `(u,v)`. Therefore, if we can find a clique of size `k\\'` in `G\\'`, it directly corresponds to a set of vertices that formed a `(k-2)`-sized clique in `G` within the neighborhood of some pair `(u,v)`. This gives us `k-2` vertices ready to be part of a near-clique, with `u` and `v` being the final two vertices to complete it.\\n\\nL: Ah, I see! The `k\\'` is set aside just for the \"inner\" clique part, not including the two vertices `u` and `v`.\\n\\nE: Precisely. Now, let\\'s look at the first direction of the proof: if `G` has a near-clique of size `k`, then `G\\'` must have a clique of size `k-2`. It says: \"let `D` be the vertices of the near-clique. Then there are two vertices `u,v \\\\in D` such that `uv \\\\notin E(G)`... and `D \\\\setminus \\\\{u,v\\\\}` forms a clique of size `k-2`, and `D \\\\subseteq N(u)\\\\cap N(v)`.\"\\n\\nE: Let\\'s dissect this. By the definition of a near-clique, the set of vertices `D` is almost a clique. The only thing stopping it from being a full clique is the single missing edge. Let\\'s call the vertices of this missing edge `u` and `v`.\\n\\nBecause `D` is a near-clique, any two vertices in it, aside from `u` and `v`, must be connected. This means the set `D \\\\setminus \\\\{u,v\\\\}`, which has `k-2` vertices, must form a sub-clique (a complete subgraph) within `G`.\\n\\nMoreover, for `D` to be a near-clique, the \"almost-clique\" condition requires that by adding the single edge `(u,v)`, the whole set `D` becomes a clique. For this to be true, `u` must be connected to every other vertex in `D` (including those in `D \\\\setminus \\\\{u,v\\\\}`), and `v` must be connected to every other vertex in `D`.\\n\\nThis means that all the vertices in `D \\\\setminus \\\\{u,v\\\\}` must be common neighbors of `u` and `v`. So, `D \\\\setminus \\\\{u,v\\\\}` is a subset of `N(u) \\\\cap N(v)`.\\n\\nL: Okay, so inside the original graph `G`, we have this `k-2` vertex clique, `D \\\\setminus \\\\{u,v\\\\}`, and every single one of these vertices is connected to `u` and `v`.\\n\\nE: Now let\\'s see what happens in `G\\'`. According to the construction process, for the non-edge `(u,v)` in `G`, we create a subgraph in `G\\'` from the set of common neighbors `N(u) \\\\cap N(v)`. Since `D \\\\setminus \\\\{u,v\\\\}` is a subset of this set, all of its vertices are represented in this subgraph of `G\\'`.\\n\\nE: Furthermore, because `D \\\\setminus \\\\{u,v\\\\}` forms a clique in `G`, all the required edges exist between its vertices. This means that in `G\\'`, where we preserve edges from `G`, the corresponding vertices will also all be connected to each other.\\n\\nTherefore, the representation of `D \\\\setminus \\\\{u,v\\\\}` forms a complete subgraph, a clique, within `G\\'`. Since this clique has `k-2` vertices, and we set `k\\' = k-2`, we have found a clique in `G\\'` of the required size.\\n\\nL: The proof goes the other way too: if `G\\'` has a `k\\'`-sized clique, then `G` has a `k`-sized near-clique. It says \"let `C\\'` be a `k\\'=k-2` size clique in `G\\'`... By the construction, there must exist `u,v\\\\in V(G)` such that `C\\\\subset N(u)\\\\cap N(v)`.\"\\nE: We pick up the argument here. The key observation is that `G\\'` is built by taking several disjoint subgraphs, one for each non-edge `(u,v)` in `G`. A clique within `G\\'` cannot have vertices from different subgraphs, because there are no edges between those separate components. So, the `k\\'`-sized clique `C\\'` must be entirely contained within one of these subgraphs.\\n\\nEvery subgraph in `G\\'` was created for a specific non-edge `(u,v)` from the original graph `G`. Therefore, if our clique `C\\'` is inside the subgraph for `(u,v)`, it means that every single vertex in `C\\'` originated from a vertex that was a common neighbor of `u` and `v` in `G`. So, if `C` is the set of original vertices corresponding to `C\\'`, then `C \\\\subset N(u) \\\\cap N(v)`. This means every vertex in `C` is connected to both `u` and `v`.\\n\\nL: That\\'s the crucial part that was confusing. The existence of the pair `(u,v)` is guaranteed because the clique `C\\'` has to live inside one of the subgraphs we built, and each subgraph was built for a specific `(u,v)` pair.\\n\\nE: Exactly. So, we have this pair `(u,v)` with no edge between them, and a set `C` of `k-2` vertices, where every vertex in `C` is connected to `u` and `v`. Now, we need to check if `C` is a clique in `G`. The document says it is, because the subgraph for `(u,v)` in `G\\'` is built by copying the edges from `G`. Since `C\\'` is a clique in `G\\'`, all the corresponding vertices in `C` must have been connected in `G`. So, `C` forms a `(k-2)`-sized clique in `G`.\\n\\nL: So now we have `u` and `v`, who are not connected, and `C`, which is a `k-2` sized clique where every vertex in `C` is connected to both `u` and `v`.\\n\\nE: Now we just assemble the near-clique. Consider the set `D = C \\\\cup \\\\{u,v\\\\}`. What are its properties?\\n1.  Its size `|D|` is `|C| + |{u,v}| = (k-2) + 2 = k`.\\n2.  The only \"missing\" edge in `D` is the one between `u` and `v`. If we were to add this edge, the set `D` would become a clique, because `u` is connected to all in `C` and `v` is connected to all in `C` (by the fact that `C \\\\subset N(u) \\\\cap N(v)`), and `C` is already a clique within itself.\\n\\nThis is precisely the definition of a `k`-sized near-clique. Therefore, we have shown that `G` contains a `k`-near-clique.\\n\\nFinally, the document points out that this whole process can be done in polynomial time. We need to examine every pair of vertices `(u,v)` in `G`. For each pair, the most expensive part of the construction is finding the intersection of their neighborhoods `N(u) \\\\cap N(v)`. If the graph has `n` vertices, there are `O(n^2)` pairs to check. Checking the neighbors for each vertex can also be done in `O(n)` time, making the total construction time `O(n^3)`, which is polynomial.'}\n",
                        "{'uid': '4b43db26-43b9-47f8-87d8-39ecabfdf218', 'content': \"Student A: Our assignment is about analyzing the area of the islands. The text mentions that there's a huge variation in size. Isabela is a massive outlier at 4600 \\\\(\\\\mathrm{mi}^2\\\\).\\n\\nStudent B: Yes, and because of that, it says the distribution of the remaining islands is still significantly skewed right. So, most of the islands are smaller.\\n\\nStudent A: Right, we can see that in the box plot without Isabela. The plot doesn't show a left whisker because it's so small.\\n\\nStudent B: That makes sense. Using the original plot would be hard to interpret because of the scale. The text gives a rescaled version to make it clearer.\\n\\nStudent A: It says to measure the center with the median, not the mean, because of the skewing and outliers.\\n\\nStudent B: Exactly. those extreme values, especially Isabela, would raise the average mean. What is the median island size?\\n\\nStudent A: The median is a good measure of the center here. The text says the median island size is approximately 42 \\\\(\\\\mathrm{square}\\\\) kilometers.\"}\n",
                        "{'uid': '3ce4d557-c021-427d-8cc5-cd2be041490e', 'content': \"Professor A: I was reviewing the homework help section and came across a problem posted by Jim155. The problem is about finding the first three terms of a sequence defined by \\\\( a_1 = 2 \\\\) and \\\\( a_n = 3(a_{n-1})^2 \\\\). Steve provided the solution, stating that the first three terms are \\\\( 2 \\\\), \\\\( 3 \\\\times 2^2 \\\\), and \\\\( 3 \\\\times (3 \\\\times 2^2)^2 \\\\), which simplify to \\\\( 2 \\\\), \\\\( 12 \\\\), and \\\\( 432 \\\\). What do you think?\\n\\nProfessor B: That's correct. Let's break it down step by step. The first term is given as \\\\( a_1 = 2 \\\\). For the second term, we use the formula \\\\( a_2 = 3(a_1)^2 \\\\). Substituting \\\\( a_1 = 2 \\\\), we get \\\\( a_2 = 3 \\\\times 2^2 = 3 \\\\times 4 = 12 \\\\).\\n\\nProfessor A: Right, and for the third term, we use \\\\( a_3 = 3(a_2)^2 \\\\). Since \\\\( a_2 = 12 \\\\), we have \\\\( a_3 = 3 \\\\times 12^2 = 3 \\\\times 144 = 432 \\\\).\\n\\nProfessor B: Exactly. So, the first three terms are indeed \\\\( 2 \\\\), \\\\( 12 \\\\), and \\\\( 432 \\\\). It's a good example of how quickly sequences can grow when they involve squaring and multiplication.\\n\\nProfessor A: Agreed. Moving on, I noticed a related question about an arithmetic sequence where the first term is 4, and the 1st, 3rd, and 7th terms are given. We need to determine the common difference. How would you approach this?\\n\\nProfessor B: To find the common difference, we can use the fact that in an arithmetic sequence, the difference between consecutive terms is constant. Let's denote the common difference by \\\\( d \\\\). The 1st term is \\\\( a_1 = 4 \\\\), the 3rd term is \\\\( a_3 = 4 + 2d \\\\), and the 7th term is \\\\( a_7 = 4 + 6d \\\\).\\n\\nProfessor A: So, we need to find \\\\( d \\\\) such that the terms fit the given sequence. If we have the values of \\\\( a_3 \\\\) and \\\\( a_7 \\\\), we can set up equations and solve for \\\\( d \\\\).\\n\\nProfessor B: Yes, if we know the values of \\\\( a_3 \\\\) and \\\\( a_7 \\\\), we can use the equations \\\\( a_3 = 4 + 2d \\\\) and \\\\( a_7 = 4 + 6d \\\\) to find \\\\( d \\\\). For example, if \\\\( a_3 = 10 \\\\) and \\\\( a_7 = 16 \\\\), we can solve \\\\( 10 = 4 + 2d \\\\) and \\\\( 16 = 4 + 6d \\\\).\\n\\nProfessor A: Solving \\\\( 10 = 4 + 2d \\\\) gives \\\\( 2d = 6 \\\\), so \\\\( d = 3 \\\\). And solving \\\\( 16 = 4 + 6d \\\\) also gives \\\\( 6d = 12 \\\\), so \\\\( d = 2 \\\\). But since these must be consistent, we should check the values provided.\\n\\nProfessor B: Exactly. If the values provided are consistent, then \\\\( d = 3 \\\\) should work for both equations. Let's move on to the next question. In another problem, the first difference of a sequence is given as \\\\( 8, 12, 16, 20, \\\\ldots \\\\). We need to find the 10th term.\\n\\nProfessor A: The first difference sequence \\\\( 8, 12, 16, 20, \\\\ldots \\\\) suggests that the sequence is quadratic. The general form of a quadratic sequence is \\\\( a_n = an^2 + bn + c \\\\). The first difference sequence is \\\\( 4n + 4 \\\\).\\n\\nProfessor B: To find the 10th term, we can use the fact that the first difference sequence is \\\\( 4n + 4 \\\\). The second difference is constant and equal to 4. We can use the formula for the sum of the first \\\\( n \\\\) terms of the first difference sequence to find the 10th term.\\n\\nProfessor A: The sum of the first \\\\( n \\\\) terms of the first difference sequence is \\\\( \\\\sum_{k=1}^{n} (4k + 4) \\\\). This can be simplified to \\\\( 4 \\\\sum_{k=1}^{n} k + 4n \\\\). Using the formula for the sum of the first \\\\( n \\\\) natural numbers, \\\\( \\\\sum_{k=1}^{n} k = \\\\frac{n(n+1)}{2} \\\\), we get \\\\( 4 \\\\left( \\\\frac{n(n+1)}{2} \\\\right) + 4n = 2n(n+1) + 4n = 2n^2 + 6n \\\\).\\n\\nProfessor B: So, the 10th term of the sequence is \\\\( 2(10)^2 + 6(10) = 200 + 60 = 260 \\\\).\\n\\nProfessor A: That's correct. The 10th term is 260. Let's look at the next question. The 1st, 5th, and 13th terms of an arithmetic sequence are given. We need to find the common difference and the nth term.\\n\\nProfessor B: Let's denote the first term by \\\\( a \\\\) and the common difference by \\\\( d \\\\). The 1st term is \\\\( a \\\\), the 5th term is \\\\( a + 4d \\\\), and the 13th term is \\\\( a + 12d \\\\). If we have the values of these terms, we can set up equations to solve for \\\\( a \\\\) and \\\\( d \\\\).\\n\\nProfessor A: For example, if the 1st term is 3, the 5th term is 11, and the 13th term is 23, we can set up the equations \\\\( 3 + 4d = 11 \\\\) and \\\\( 3 + 12d = 23 \\\\).\\n\\nProfessor B: Solving \\\\( 3 + 4d = 11 \\\\) gives \\\\( 4d = 8 \\\\), so \\\\( d = 2 \\\\). And solving \\\\( 3 + 12d = 23 \\\\) gives \\\\( 12d = 20 \\\\), so \\\\( d = 2 \\\\). The common difference is \\\\( d = 2 \\\\).\\n\\nProfessor A: The nth term of the sequence is given by \\\\( a_n = a + (n-1)d \\\\). Substituting \\\\( a = 3 \\\\) and \\\\( d = 2 \\\\), we get \\\\( a_n = 3 + (n-1)2 = 3 + 2n - 2 = 2n + 1 \\\\).\\n\\nProfessor B: So, the nth term is \\\\( 2n + 1 \\\\). Let's move on to the next question. We need to determine whether a sequence is arithmetic or geometric and find the nth term.\\n\\nProfessor A: To determine the type of sequence, we need to check if the ratio between consecutive terms is constant (geometric) or if the difference between consecutive terms is constant (arithmetic).\\n\\nProfessor B: For example, if the sequence is \\\\( 2, 6, 18, 54, \\\\ldots \\\\), the ratio between consecutive terms is \\\\( \\\\frac{6}{2} = 3 \\\\), \\\\( \\\\frac{18}{6} = 3 \\\\), and \\\\( \\\\frac{54}{18} = 3 \\\\). Since the ratio is constant, the sequence is geometric with a common ratio of 3.\\n\\nProfessor A: The nth term of a geometric sequence is given by \\\\( a_n = a \\\\cdot r^{n-1} \\\\). If the first term \\\\( a = 2 \\\\) and the common ratio \\\\( r = 3 \\\\), the nth term is \\\\( a_n = 2 \\\\cdot 3^{n-1} \\\\).\\n\\nProfessor B: That's correct. Let's look at the next question. The first and second terms of an exponential sequence (geometric progression, G.P.) are given. We need to find the common ratio.\\n\\nProfessor A: If the first term is \\\\( a \\\\) and the second term is \\\\( ar \\\\), the common ratio \\\\( r \\\\) can be found by dividing the second term by the first term.\\n\\nProfessor B: For example, if the first term is 5 and the second term is 15, the common ratio \\\\( r \\\\) is \\\\( \\\\frac{15}{5} = 3 \\\\).\\n\\nProfessor A: So, the common ratio is 3. Let's move on to the next question. In an arithmetic sequence, the common difference is equal to 2, and the first term is given. We need to find the nth term.\\n\\nProfessor B: The nth term of an arithmetic sequence is given by \\\\( a_n = a + (n-1)d \\\\). If the first term \\\\( a = 3 \\\\) and the common difference \\\\( d = 2 \\\\), the nth term is \\\\( a_n = 3 + (n-1)2 = 3 + 2n - 2 = 2n + 1 \\\\).\\n\\nProfessor A: So, the nth term is \\\\( 2n + 1 \\\\). Let's look at the next question. The sum of the first nine terms of an arithmetic sequence is 216. The 1st, 4th, and 9th terms are given. We need to find the common difference.\\n\\nProfessor B: The sum of the first nine terms of an arithmetic sequence is given by \\\\( S_9 = \\\\frac{9}{2} (2a + 8d) = 216 \\\\). Simplifying, we get \\\\( 9(2a + 8d) = 432 \\\\), so \\\\( 2a + 8d = 48 \\\\), or \\\\( a + 4d = 24 \\\\).\\n\\nProfessor A: If the 1st term is \\\\( a \\\\), the 4th term is \\\\( a + 3d \\\\), and the 9th term is \\\\( a + 8d \\\\), we can set up equations using the given terms.\\n\\nProfessor B: For example, if the 1st term is 4, the 4th term is 16, and the 9th term is 28, we can set up the equations \\\\( 4 + 3d = 16 \\\\) and \\\\( 4 + 8d = 28 \\\\).\\n\\nProfessor A: Solving \\\\( 4 + 3d = 16 \\\\) gives \\\\( 3d = 12 \\\\), so \\\\( d = 4 \\\\). And solving \\\\( 4 + 8d = 28 \\\\) gives \\\\( 8d = 24 \\\\), so \\\\( d = 3 \\\\). Since these must be consistent, we should check the values provided.\\n\\nProfessor B: Exactly. If the values provided are consistent, then \\\\( d = 4 \\\\) should work for both equations. Let's move on to the next question. The first three terms of a geometric sequence are the same as the 1st, 9th, and 27th terms of an arithmetic sequence. We need to find the common ratio.\\n\\nProfessor A: Let the first term of the geometric sequence be \\\\( a \\\\) and the common ratio be \\\\( r \\\\). The first three terms of the geometric sequence are \\\\( a \\\\), \\\\( ar \\\\), and \\\\( ar^2 \\\\).\\n\\nProfessor B: Let the first term of the arithmetic sequence be \\\\( A \\\\) and the common difference be \\\\( D \\\\). The 1st term is \\\\( A \\\\), the 9th term is \\\\( A + 8D \\\\), and the 27th term is \\\\( A + 26D \\\\).\\n\\nProfessor A: Setting these equal, we get \\\\( a = A \\\\), \\\\( ar = A + 8D \\\\), and \\\\( ar^2 = A + 26D \\\\).\\n\\nProfessor B: Solving these equations, we can find \\\\( r \\\\). For example, if \\\\( a = 2 \\\\), \\\\( ar = 10 \\\\), and \\\\( ar^2 = 26 \\\\), we can solve \\\\( 2r = 10 \\\\) to get \\\\( r = 5 \\\\) and \\\\( 2r^2 = 26 \\\\) to get \\\\( r = \\\\sqrt{13} \\\\). Since these must be consistent, we should check the values provided.\\n\\nProfessor A: Exactly. If the values provided are consistent, then \\\\( r = 3 \\\\) should work for both equations. Let's move on to the next question. A student is working on some nth term problems and wants to know if their solution is correct.\\n\\nProfessor B: We should review their work and provide feedback. If they have the correct approach and calculations, we can confirm their solution.\\n\\nProfessor A: Let's look at the final question. We need to find the next three terms in the sequence: \\\\( 1, 30, 22, 14, 6, -3, -12, -21, \\\\ldots \\\\).\\n\\nProfessor B: To find the next three terms, we need to identify the pattern in the sequence. The differences between consecutive terms are \\\\( 29, -8, -8, -8, -9, -9, -9 \\\\).\\n\\nProfessor A: The differences suggest that the sequence is not arithmetic or geometric. However, the differences between the differences are \\\\( -37, 0, 0, 0, 0, 0 \\\\), indicating a pattern.\\n\\nProfessor B: The next differences should follow the same pattern. So, the next differences are \\\\( -9, -9, -9 \\\\). Adding these to the last term, we get \\\\( -21 - 9 = -30 \\\\), \\\\( -30 - 9 = -39 \\\\), and \\\\( -39 - 9 = -48 \\\\).\\n\\nProfessor A: So, the next three terms are \\\\( -30, -39, -48 \\\\).\\n\\nProfessor B: That's correct. The next three terms are \\\\( -30, -39, -48 \\\\).\"}\n"
                    ]
                }
            ],
            "source": [
                "# Load library\n",
                "from datasets import load_dataset\n",
                "\n",
                "print(\"Loading UltraData-Math dataset in streaming mode...\")\n",
                "\n",
                "# Load training split with a specified config name in streaming mode\n",
                "dataset = load_dataset(\"openbmb/UltraData-Math\", name=\"UltraData-Math-L3-Conversation-Synthetic\", split=\"train\", streaming=True)\n",
                "\n",
                "print(f\"\\nDataset features:\")\n",
                "print(dataset.features)\n",
                "\n",
                "print(\"\\nFirst 5 examples (from streaming data):\")\n",
                "# Iterate and print first few examples from the streaming dataset\n",
                "for i, example in enumerate(dataset):\n",
                "    print(example)\n",
                "    if i >= 4: # Print 5 examples\n",
                "        break\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "e6XgYNRDMC7L"
            },
            "source": [
                "**3. Data Preprocessing**\n",
                "\n",
                "Convert the dataset into instruction-response format suitable for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "FJw3pyoOwF3P"
            },
            "outputs": [],
            "source": [
                "# Define formatting function\n",
                "def format_instruction(example):\n",
                "    \"\"\"\n",
                "    Format dataset into instruction-response text, specifically for conversational math data.\n",
                "    It extracts the first student/user turn as the instruction and the rest of the dialogue as the response.\n",
                "    \"\"\"\n",
                "\n",
                "    instruction = None\n",
                "    response = None\n",
                "\n",
                "    if \"content\" in example: # Specific handling for UltraData-Math's 'content' field\n",
                "        dialogue = example[\"content\"]\n",
                "\n",
                "        # Find the first occurrence of \"Student:\" or \"User:\"\n",
                "        student_idx = dialogue.find(\"Student:\")\n",
                "        user_idx = dialogue.find(\"User:\")\n",
                "\n",
                "        first_speaker_idx = -1\n",
                "        speaker_tag_len = 0\n",
                "\n",
                "        if student_idx != -1 and (user_idx == -1 or student_idx < user_idx):\n",
                "            first_speaker_idx = student_idx\n",
                "            speaker_tag_len = len(\"Student:\")\n",
                "        elif user_idx != -1 and (student_idx == -1 or user_idx < student_idx):\n",
                "            first_speaker_idx = user_idx\n",
                "            speaker_tag_len = len(\"User:\")\n",
                "\n",
                "        if first_speaker_idx != -1:\n",
                "            # Find the start of the first response from a teacher/Qwen after the initial instruction\n",
                "            teacher_idx = dialogue.find(\"Teacher:\", first_speaker_idx + speaker_tag_len)\n",
                "            qwen_idx = dialogue.find(\"Qwen:\", first_speaker_idx + speaker_tag_len)\n",
                "\n",
                "            response_start_idx = -1\n",
                "            if teacher_idx != -1 and (qwen_idx == -1 or teacher_idx < qwen_idx):\n",
                "                response_start_idx = teacher_idx\n",
                "            elif qwen_idx != -1 and (teacher_idx == -1 or qwen_idx < teacher_idx):\n",
                "                response_start_idx = qwen_idx\n",
                "\n",
                "            if response_start_idx != -1:\n",
                "                instruction = dialogue[:response_start_idx].strip()\n",
                "                response = dialogue[response_start_idx:].strip()\n",
                "            else:\n",
                "                # If no clear teacher/Qwen response found, treat the whole dialogue as instruction\n",
                "                instruction = dialogue.strip()\n",
                "                response = \"\"\n",
                "        else:\n",
                "            # Fallback if no specific speaker turn is found (e.g., malformed content)\n",
                "            instruction = \"Analyze the following math dialogue:\"\n",
                "            response = dialogue.strip()\n",
                "\n",
                "    # Fallback for other potential datasets with standard instruction/output fields\n",
                "    elif \"instruction\" in example and \"output\" in example:\n",
                "        instruction = example[\"instruction\"]\n",
                "        response = example[\"output\"]\n",
                "\n",
                "    elif \"question\" in example and \"answer\" in example:\n",
                "        instruction = example[\"question\"]\n",
                "        response = example[\"answer\"]\n",
                "\n",
                "    elif \"input\" in example and \"output\" in example:\n",
                "        instruction = example[\"input\"]\n",
                "        response = example[\"output\"]\n",
                "\n",
                "    # If after all attempts, instruction or response is still None, return an invalid example\n",
                "    if instruction is None or response is None:\n",
                "        return {\"text\": None}\n",
                "\n",
                "    formatted_text = (\n",
                "        \"### Instruction:\\n\"\n",
                "        f\"{instruction}\\n\\n\"\n",
                "        \"### Response:\\n\"\n",
                "        f\"{response}\"\n",
                "    )\n",
                "    return {\"text\": formatted_text}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "LHFVHwbCwTyd",
                "outputId": "8d86c7f8-51b8-4f7e-d288-c4a01621b65f"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Formatting dataset...\n",
                        "\n",
                        "Example formatted text (first example from stream):\n",
                        "\n",
                        "### Instruction:\n",
                        "Student: I'm having trouble understanding the problem statement for the integral \\(\\int_{0}^{2\\pi}(x-\\pi)^2 (\\sin x) dx\\). Could you explain it to me step-by-step?\n",
                        "\n",
                        "### Response:\n",
                        "Teacher: Of course! Let's break it down. The problem is to evaluate the integral \\(\\int_{0}^{2\\pi}(x-\\pi)^2 (\\sin x) dx\\).\n",
                        "\n",
                        "Student: Okay, I see the integral. But why do we need to use substitution here?\n",
                        "\n",
                        "Teacher: Good question. Using substitution can simplify the integral. We let \\( t = x - \\pi \\). Thi\n"
                    ]
                }
            ],
            "source": [
                "print(\"Formatting dataset...\")\n",
                "\n",
                "formatted_dataset = dataset.map(\n",
                "    format_instruction,\n",
                "    remove_columns=dataset.column_names\n",
                ")\n",
                "\n",
                "# Remove failed examples\n",
                "formatted_dataset = formatted_dataset.filter(\n",
                "    lambda x: x[\"text\"] is not None\n",
                ")\n",
                "\n",
                "# For streaming datasets, we cannot directly get the length or access by index.\n",
                "# We can iterate to get a few examples.\n",
                "print(\"\\nExample formatted text (first example from stream):\\n\")\n",
                "for i, example in enumerate(formatted_dataset):\n",
                "    print(example[\"text\"][:500])\n",
                "    if i == 0: # Print only the first example\n",
                "        break"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "VDw2U2P7x0dC",
                "outputId": "3fc8d208-29ed-4529-8230-23ec1d058084"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting 3000 examples from the streaming dataset...\n",
                        "Materialized dataset size: 3000 examples\n",
                        "\n",
                        "Training examples: 2700\n",
                        "Validation examples: 300\n"
                    ]
                }
            ],
            "source": [
                "# Select subset for efficient training\n",
                "DATASET_SIZE = 3000  # Adjust depending on GPU memory\n",
                "\n",
                "# For streaming datasets, we cannot directly use len(), shuffle() or train_test_split()\n",
                "# if we want to limit the total number of examples and then split.\n",
                "# We will take DATASET_SIZE examples from the stream and then convert it to a regular Dataset\n",
                "# to enable shuffling and splitting.\n",
                "\n",
                "print(f\"Collecting {DATASET_SIZE} examples from the streaming dataset...\")\n",
                "# Using `take(DATASET_SIZE)` to get the first N examples from the stream.\n",
                "# Then convert to a regular Dataset.\n",
                "# Note: `shuffle()` on an IterableDataset shuffles the *order of chunks*, not individual examples.\n",
                "# To get a truly random subset and then split, it's better to materialize a limited number.\n",
                "limited_dataset_list = []\n",
                "for i, example in enumerate(formatted_dataset):\n",
                "    if i >= DATASET_SIZE:\n",
                "        break\n",
                "    limited_dataset_list.append(example)\n",
                "\n",
                "# Convert the list of examples to a regular Dataset\n",
                "from datasets import Dataset\n",
                "materialized_dataset = Dataset.from_list(limited_dataset_list)\n",
                "print(f\"Materialized dataset size: {len(materialized_dataset)} examples\")\n",
                "\n",
                "# Now we can shuffle and split the materialized dataset\n",
                "shuffled_dataset = materialized_dataset.shuffle(seed=42)\n",
                "\n",
                "# Split into train and validation\n",
                "split_dataset = shuffled_dataset.train_test_split(\n",
                "    test_size=0.1,\n",
                "    seed=42\n",
                ")\n",
                "\n",
                "train_dataset = split_dataset[\"train\"]\n",
                "eval_dataset = split_dataset[\"test\"]\n",
                "\n",
                "print(f\"\\nTraining examples: {len(train_dataset)}\")\n",
                "print(f\"Validation examples: {len(eval_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ffhkvQuuMWOb"
            },
            "source": [
                "**4. Model Selection and LoRA Configuration**\n",
                "\n",
                "Load TinyLlama model and configure LoRA for parameter-efficient fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "id": "utKDD5ZOyfu6"
            },
            "outputs": [],
            "source": [
                "!  pip install -U bitsandbytes>=0.46.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "m4CZFy56zRRJ",
                "outputId": "2633cd8e-c0fa-4e60-c159-98a3edec284b"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.2)\n",
                        "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.10.0+cu128)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
                        "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.24.2)\n",
                        "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
                        "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
                        "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
                        "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.9.4)\n",
                        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
                        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
                        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
                        "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
                        "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
                        "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
                        "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
                        "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
                        "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
                        "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
                        "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
                        "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.5)\n",
                        "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
                        "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
                        "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
                        "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.0)\n",
                        "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch<3,>=2.3->bitsandbytes) (1.3.4)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
                    ]
                }
            ],
            "source": [
                "! pip install -U bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "fuzhXf-fz91y",
                "outputId": "183dafbf-a6f1-473d-8924-ddc35f87b69d"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.2)\n",
                        "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
                        "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.10.0+cu128)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
                        "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0)\n",
                        "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
                        "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
                        "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (1.4.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.24.2)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
                        "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (0.28.1)\n",
                        "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.5.4)\n",
                        "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.3)\n",
                        "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (0.24.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
                        "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
                        "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.9.4)\n",
                        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
                        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
                        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
                        "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
                        "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
                        "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
                        "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
                        "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
                        "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
                        "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
                        "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
                        "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.5)\n",
                        "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
                        "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
                        "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
                        "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.0)\n",
                        "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch<3,>=2.3->bitsandbytes) (1.3.4)\n",
                        "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (4.12.1)\n",
                        "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (2026.1.4)\n",
                        "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.0.9)\n",
                        "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
                        "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (0.16.0)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
                        "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate) (0.24.0)\n",
                        "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate) (8.3.1)\n",
                        "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate) (13.9.4)\n",
                        "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate) (0.0.4)\n",
                        "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate) (4.0.0)\n",
                        "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate) (2.19.2)\n",
                        "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface_hub>=0.21.0->accelerate) (0.1.2)\n"
                    ]
                }
            ],
            "source": [
                "!pip install -U bitsandbytes accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 165,
                    "referenced_widgets": [
                        "2839a149a12b47cf8f901f16f477648a",
                        "ed1a8081b2cd4071a0493a943907f4e6",
                        "85605481f7b847e39187a97b0fdf82d3",
                        "cba3b109d7a94043bfd77d8442ed9bb5",
                        "a5c2683f94574231b8fda42a31c6c373",
                        "de16f9f7507e43d8911aad06c3e6d455",
                        "5c6dda4d3aca46cf997a13feade9f270",
                        "cbf2ac102349469dac03121a7f5c7fd0",
                        "ef74351f21c643e695001462e1926445",
                        "68512d84a92341bd838dfe972a03e87a",
                        "969ae1438b1449949ebe0a0417fbb5c5",
                        "a18ad8f79510485fa22e10de8524dd6e",
                        "59bf7a715c464d1a937b5c34b98505b3",
                        "dad396fe5c9748dc91e56cc782ec3696",
                        "227cccae9fad4bb6b45aaf907fc36616",
                        "863cc10f0ad1484fbdbb1428ab66e05f",
                        "228961c521a149b7aca2a396f7d37bdf",
                        "956280f7b9ac40e78f59f29694e907c8",
                        "9338fa2c018b4987b076a3095b5be435",
                        "f8a816142bb84c1ba67f8a564a4999c9",
                        "725f52a8dce04e4db53a37b0c1c59f8b",
                        "514d5b556943453d841393230ee853ed",
                        "c04d95aef0e1414098eef9562f620f37",
                        "7e03377f71c1444db0c823787b5aa6c2",
                        "28c330e61cc1432d9bba2fb2a0287b52",
                        "4696e134e37043a9a075bcfc7b876d0e",
                        "0ef6679c5a704c809c396d209d0e7fbf",
                        "98f506de46444cefb05d1eb3fd82e34e",
                        "8a148c4118b44545b1a551c2a98bd3bd",
                        "0cbb93e565a643b19f9991ee6b700554",
                        "0ee36dc3282b48b589a86564836545c7",
                        "df1dcf37bd3d417fa3cd9d3ac961f5a3",
                        "b4d2b4740caa44dba243d563d04964c2"
                    ]
                },
                "id": "7CoNSXCGyYJZ",
                "outputId": "e8ab1f9b-b1d5-4c2a-c954-1e1b87892f90"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2839a149a12b47cf8f901f16f477648a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a18ad8f79510485fa22e10de8524dd6e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c04d95aef0e1414098eef9562f620f37",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model loaded successfully!\n",
                        "Total parameters: 1100.05M\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForCausalLM,\n",
                "    BitsAndBytesConfig\n",
                ")\n",
                "from peft import (\n",
                "    LoraConfig,\n",
                "    get_peft_model,\n",
                "    TaskType,\n",
                "    prepare_model_for_kbit_training\n",
                ")\n",
                "\n",
                "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "\n",
                "print(f\"Loading model: {MODEL_NAME}\")\n",
                "\n",
                "# ---- 4-bit quantization config (modern way) ----\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\"\n",
                ")\n",
                "\n",
                "# ---- Load tokenizer ----\n",
                "tokenizer = AutoTokenizer.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "# Fix padding token if missing\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "# ---- Load model ----\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "# Prepare for k-bit training\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "print(\"Model loaded successfully!\")\n",
                "print(f\"Total parameters: {model.num_parameters() / 1e6:.2f}M\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "9mB9ESve1JKx",
                "outputId": "6cc5b5b1-e9e2-4bc9-bd54-022e71acf950"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n",
                        "\n",
                        "LoRA configuration applied successfully!\n"
                    ]
                }
            ],
            "source": [
                "# LoRA configuration\n",
                "lora_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\n",
                "        \"q_proj\",\n",
                "        \"k_proj\",\n",
                "        \"v_proj\",\n",
                "        \"o_proj\"\n",
                "    ],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=TaskType.CAUSAL_LM\n",
                ")\n",
                "\n",
                "# Apply LoRA\n",
                "model = get_peft_model(model, lora_config)\n",
                "\n",
                "model.print_trainable_parameters()\n",
                "\n",
                "print(\"\\nLoRA configuration applied successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 116,
                    "referenced_widgets": [
                        "2bcf4716f9b84d368b860a904b45d555",
                        "18821952f0474f4aaed471990d78d658",
                        "816fb06cb7fe40f2a3e6060a7596fe8a",
                        "b0d212c8d4674decacb88b5b448a4604",
                        "74dd2f3b443f4cc2a861e8c02cf2fe79",
                        "6ec4347c426d4e5096dff4814f5999e9",
                        "b45da473fb0b4af583287c48ba63f16f",
                        "de6ce67451464116ac221daced61eefb",
                        "b57fbaa69a484ee99a5d448f81798e0f",
                        "eba0c93c36a14d8297e7ddc00481256f",
                        "4c6fff27c35d4702af4a14104ea87fc6",
                        "744b347e3a624b10b1705ff85579be89",
                        "75a2b640685849cf9f83f3c3b34ca417",
                        "424d7800516c46d69e9b07cdbe08117b",
                        "0bb12ad7064b4cdbb3249211e147df54",
                        "2c13e8500b8443778717ccf9a26bd766",
                        "d35326e8e3954d3bbb71dabe720a7f98",
                        "78083c649dc04be38d0520ca57b1d73a",
                        "b7dd91afa26a4cb587b929cc86d67b93",
                        "6a6143bdbe21468d8526acda1e9905cd",
                        "63bef46d20044c89bbc28725b08bfda2",
                        "bc11c0a9bcb04fd084294e8726a962e9"
                    ]
                },
                "id": "lbkrr97q1Qku",
                "outputId": "77a9d47a-0556-4bf7-9f22-5776e75abede"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokenizing datasets...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2bcf4716f9b84d368b860a904b45d555",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "744b347e3a624b10b1705ff85579be89",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokenization complete!\n"
                    ]
                }
            ],
            "source": [
                "MAX_LENGTH = 300\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    tokenized = tokenizer(\n",
                "        examples[\"text\"],\n",
                "        padding=\"max_length\",\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH\n",
                "    )\n",
                "\n",
                "    # Important for causal LM\n",
                "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
                "\n",
                "    return tokenized\n",
                "\n",
                "\n",
                "print(\"Tokenizing datasets...\")\n",
                "\n",
                "tokenized_train = train_dataset.map(\n",
                "    tokenize_function,\n",
                "    batched=True,\n",
                "    remove_columns=train_dataset.column_names\n",
                ")\n",
                "\n",
                "tokenized_eval = eval_dataset.map(\n",
                "    tokenize_function,\n",
                "    batched=True,\n",
                "    remove_columns=eval_dataset.column_names\n",
                ")\n",
                "\n",
                "print(\"Tokenization complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "fnadfqX-1-7E",
                "outputId": "81baba11-7e92-4a89-8388-fc3dd0c33baf"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Experiment tracking table created!\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "experiment_results = []\n",
                "\n",
                "def create_experiment_table():\n",
                "    columns = [\n",
                "        \"Experiment\",\n",
                "        \"Learning Rate\",\n",
                "        \"Batch Size\",\n",
                "        \"Gradient Accumulation\",\n",
                "        \"Epochs\",\n",
                "        \"LoRA Rank\",\n",
                "        \"Final Train Loss\",\n",
                "        \"Final Eval Loss\",\n",
                "        \"Training Time (min)\",\n",
                "        \"GPU Memory (GB)\",\n",
                "        \"Notes\"\n",
                "    ]\n",
                "    return pd.DataFrame(columns=columns)\n",
                "\n",
                "experiment_table = create_experiment_table()\n",
                "print(\"Experiment tracking table created!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dB3rda-6Mnj9"
            },
            "source": [
                "**5. Training with Hyperparameter Experiments**\n",
                "\n",
                "We'll conduct multiple training experiments with different hyperparameters and document results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "id": "JpeFIIjN2Ar_"
            },
            "outputs": [],
            "source": [
                "import time\n",
                "import torch\n",
                "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
                "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
                "\n",
                "def run_experiment(\n",
                "    experiment_name,\n",
                "    learning_rate,\n",
                "    batch_size,\n",
                "    gradient_accumulation_steps,\n",
                "    num_epochs,\n",
                "    lora_rank,\n",
                "    notes=\"\"\n",
                "):\n",
                "\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(f\"Starting {experiment_name}\")\n",
                "    print(\"=\"*60)\n",
                "\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "    # ---- 4-bit config ----\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_compute_dtype=torch.float16,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_use_double_quant=True,\n",
                "    )\n",
                "\n",
                "    # ---- Reload fresh model ----\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "    # ---- Apply LoRA ----\n",
                "    lora_config = LoraConfig(\n",
                "        r=lora_rank,\n",
                "        lora_alpha=32,\n",
                "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
                "        lora_dropout=0.05,\n",
                "        bias=\"none\",\n",
                "        task_type=TaskType.CAUSAL_LM\n",
                "    )\n",
                "\n",
                "    model = get_peft_model(model, lora_config)\n",
                "\n",
                "    # ---- Training arguments ----\n",
                "    training_args = TrainingArguments(\n",
                "        output_dir=f\"./results/{experiment_name}\",\n",
                "        num_train_epochs=num_epochs,\n",
                "        per_device_train_batch_size=batch_size,\n",
                "        per_device_eval_batch_size=batch_size,\n",
                "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
                "        learning_rate=learning_rate,\n",
                "        fp16=True,\n",
                "        logging_steps=50,\n",
                "        eval_strategy=\"steps\",\n",
                "        eval_steps=200,\n",
                "        save_strategy=\"steps\",\n",
                "        save_steps=400,\n",
                "        save_total_limit=2,\n",
                "        load_best_model_at_end=True,\n",
                "        report_to=\"none\",\n",
                "        warmup_ratio=0.05,\n",
                "        lr_scheduler_type=\"cosine\"\n",
                "    )\n",
                "\n",
                "    data_collator = DataCollatorForLanguageModeling(\n",
                "        tokenizer=tokenizer,\n",
                "        mlm=False\n",
                "    )\n",
                "\n",
                "    trainer = Trainer(\n",
                "        model=model,\n",
                "        args=training_args,\n",
                "        train_dataset=tokenized_train,\n",
                "        eval_dataset=tokenized_eval,\n",
                "        data_collator=data_collator\n",
                "    )\n",
                "\n",
                "    # ---- Train ----\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.reset_peak_memory_stats()\n",
                "\n",
                "    start_time = time.time()\n",
                "    train_result = trainer.train()\n",
                "    training_time = (time.time() - start_time) / 60\n",
                "\n",
                "    final_train_loss = train_result.training_loss\n",
                "    eval_results = trainer.evaluate()\n",
                "    final_eval_loss = eval_results[\"eval_loss\"]\n",
                "\n",
                "    if torch.cuda.is_available():\n",
                "        gpu_memory = torch.cuda.max_memory_allocated() / 1e9\n",
                "    else:\n",
                "        gpu_memory = 0\n",
                "\n",
                "    # ---- Store results ----\n",
                "    result_row = {\n",
                "        \"Experiment\": experiment_name,\n",
                "        \"Learning Rate\": learning_rate,\n",
                "        \"Batch Size\": batch_size,\n",
                "        \"Gradient Accumulation\": gradient_accumulation_steps,\n",
                "        \"Epochs\": num_epochs,\n",
                "        \"LoRA Rank\": lora_rank,\n",
                "        \"Final Train Loss\": round(final_train_loss, 4),\n",
                "        \"Final Eval Loss\": round(final_eval_loss, 4),\n",
                "        \"Training Time (min)\": round(training_time, 2),\n",
                "        \"GPU Memory (GB)\": round(gpu_memory, 2),\n",
                "        \"Notes\": notes\n",
                "    }\n",
                "\n",
                "    global experiment_table\n",
                "    experiment_table = pd.concat(\n",
                "        [experiment_table, pd.DataFrame([result_row])],\n",
                "        ignore_index=True\n",
                "    )\n",
                "\n",
                "    trainer.save_model(f\"./models/{experiment_name}\")\n",
                "\n",
                "    print(f\"\\n{experiment_name} completed!\")\n",
                "    print(f\"Training Loss: {final_train_loss:.4f}\")\n",
                "    print(f\"Validation Loss: {final_eval_loss:.4f}\")\n",
                "    print(f\"Training Time: {training_time:.2f} min\")\n",
                "\n",
                "    return trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 363,
                    "referenced_widgets": [
                        "7c93c3f37dcc4e79a06e690471b9711a",
                        "4c6645c9aa65420292f76d3f6eff374b",
                        "1ce5649d63c9402e9609ed41eb402808",
                        "3979256b9602493fa0f2cb6334973d57",
                        "b5bcd77d0b8744b8b6b4916a43f2bb42",
                        "1e7d463b60864159ba71a3b6e9f77a67",
                        "abde6c84d8ab47b5bb80fa90cac06679",
                        "084243f2bd544be7a95618d6bdae8ae1",
                        "e75bf8d4a9c1458b86c4d6ccb74613b9",
                        "0092bb211e0e4027a526c7d0ad9f5379",
                        "bcf8c9450989471199129a92cf04da62"
                    ]
                },
                "id": "7HGTRB_r2L_o",
                "outputId": "92ce932f-433c-4fb5-b096-cb91dddf41b5"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Starting Exp1_Baseline\n",
                        "============================================================\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7c93c3f37dcc4e79a06e690471b9711a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [507/507 34:41, Epoch 3/3]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>200</td>\n",
                            "      <td>1.181927</td>\n",
                            "      <td>1.193019</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>400</td>\n",
                            "      <td>1.112981</td>\n",
                            "      <td>1.177031</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [75/75 00:23]\n",
                            "    </div>\n",
                            "    "
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Exp1_Baseline completed!\n",
                        "Training Loss: 1.1664\n",
                        "Validation Loss: 1.1770\n",
                        "Training Time: 34.79 min\n"
                    ]
                }
            ],
            "source": [
                "trainer_exp1 = run_experiment(\n",
                "    experiment_name=\"Exp1_Baseline\",\n",
                "    learning_rate=2e-4,\n",
                "    batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    num_epochs=3,\n",
                "    lora_rank=16,\n",
                "    notes=\"Baseline configuration\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 363,
                    "referenced_widgets": [
                        "e1dc522f686e44238a978c1ee8c27042",
                        "6d62d37d70e94f09b300aaf5491e0125",
                        "c97cfdeb361041f0ae96e2a3e39c1254",
                        "4cfebcca0124411f988c5d21440261e2",
                        "99d64b442f9b43449320dfa6c7574f5d",
                        "fa38209295a941c580473f3832162033",
                        "6c3be3e9d6dc46408682938fe48c833f",
                        "dd2aedc141014fc7ba2a22324969b9bc",
                        "fcefa41a757d46fdbdfb90a33323db4a",
                        "61ea6f91bd5043cdb51e2b0d7167c6ea",
                        "e79206a0974a4e44b5da5a8d8bf26037"
                    ]
                },
                "id": "7oJAcs-Y-91Q",
                "outputId": "6cdfe44a-d835-412f-fc1c-74a3a9e1e58e"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Starting Exp2_LowerLR\n",
                        "============================================================\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e1dc522f686e44238a978c1ee8c27042",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [507/507 34:37, Epoch 3/3]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>200</td>\n",
                            "      <td>1.205436</td>\n",
                            "      <td>1.208861</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>400</td>\n",
                            "      <td>1.157011</td>\n",
                            "      <td>1.192853</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [75/75 00:23]\n",
                            "    </div>\n",
                            "    "
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Exp2_LowerLR completed!\n",
                        "Training Loss: 1.1994\n",
                        "Validation Loss: 1.1929\n",
                        "Training Time: 34.71 min\n"
                    ]
                }
            ],
            "source": [
                "trainer_exp2 = run_experiment(\n",
                "    experiment_name=\"Exp2_LowerLR\",\n",
                "    learning_rate=1e-4,\n",
                "    batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    num_epochs=3,\n",
                "    lora_rank=16,\n",
                "    notes=\"Lower learning rate\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 363,
                    "referenced_widgets": [
                        "61aaf23cbb684141b0dfa796a268b1f3",
                        "0edf0784c3a94bff92947faaf0808ec7",
                        "93c686c3caeb4a6aa72849da62850741",
                        "fe3eccf89e7e45c0aa9f40377a423f06",
                        "bc760e7ea408449a83c090fc5e67fc1c",
                        "8b0bb822c19342688b0f8e3096fdefc1",
                        "f42ed7b1236747278d46c0862ab99f02",
                        "7b93429847ea4099b410a1ac798dba00",
                        "3f88f94502d24b3ab2a5201e5d546c55",
                        "1ee8c5cea12f4abe9d296942499476e4",
                        "096488321be9493387bfe03d856ee5dc"
                    ]
                },
                "id": "B_wzq5m_HHTQ",
                "outputId": "317980e4-d61d-4a0c-d215-666b0a33dfab"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Starting Exp3_HigherRank\n",
                        "============================================================\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "61aaf23cbb684141b0dfa796a268b1f3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [507/507 35:09, Epoch 3/3]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>200</td>\n",
                            "      <td>1.181142</td>\n",
                            "      <td>1.192468</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>400</td>\n",
                            "      <td>1.110612</td>\n",
                            "      <td>1.176073</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [75/75 00:23]\n",
                            "    </div>\n",
                            "    "
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Exp3_HigherRank completed!\n",
                        "Training Loss: 1.1651\n",
                        "Validation Loss: 1.1761\n",
                        "Training Time: 35.26 min\n"
                    ]
                }
            ],
            "source": [
                "trainer_exp3 = run_experiment(\n",
                "    experiment_name=\"Exp3_HigherRank\",\n",
                "    learning_rate=2e-4,\n",
                "    batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    num_epochs=3,\n",
                "    lora_rank=32,\n",
                "    notes=\"Higher LoRA rank\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ZFr13T4yM9Q6",
                "outputId": "3357f65e-6594-4500-debe-23dd3af52d3e"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "====================================================================================================\n",
                        "EXPERIMENT RESULTS SUMMARY\n",
                        "====================================================================================================\n",
                        "     Experiment  Learning Rate Batch Size Gradient Accumulation Epochs LoRA Rank  Final Train Loss  Final Eval Loss  Training Time (min)  GPU Memory (GB)                  Notes\n",
                        "  Exp1_Baseline         0.0002          4                     4      3        16            1.1664           1.1770                34.79             4.07 Baseline configuration\n",
                        "   Exp2_LowerLR         0.0001          4                     4      3        16            1.1994           1.1929                34.71             5.17    Lower learning rate\n",
                        "Exp3_HigherRank         0.0002          4                     4      3        32            1.1651           1.1761                35.26             6.34       Higher LoRA rank\n",
                        "\n",
                        "Results saved to 'experiment_results.csv'\n"
                    ]
                }
            ],
            "source": [
                "# Display experiment results table\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
                "print(\"=\"*100)\n",
                "print(experiment_table.to_string(index=False))\n",
                "\n",
                "# Save to CSV\n",
                "experiment_table.to_csv('experiment_results.csv', index=False)\n",
                "print(\"\\nResults saved to 'experiment_results.csv'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TzH_DgQ8Myn-"
            },
            "source": [
                "**6. Evaluation Metrics**\n",
                "\n",
                "Evaluate the best model using ROUGE, BLEU, and perplexity metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "CO8TAsVaPlwR",
                "outputId": "5e7af3a1-f19f-4c57-b32f-e088f156d9bc"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting rouge_score\n",
                        "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
                        "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
                        "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
                        "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
                        "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
                        "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
                        "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.3)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2025.11.3)\n",
                        "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.3)\n",
                        "Building wheels for collected packages: rouge_score\n",
                        "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=62ef01a6be18618149c689dbcc2cf2ac0539ffa6717f0bb38186d073ef6df233\n",
                        "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
                        "Successfully built rouge_score\n",
                        "Installing collected packages: rouge_score\n",
                        "Successfully installed rouge_score-0.1.2\n"
                    ]
                }
            ],
            "source": [
                "! pip install rouge_score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "GfQJQTtwPvw3",
                "outputId": "c4977f2b-65b4-4b55-9302-5ed435b47e2f"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting sacrebleu\n",
                        "  Downloading sacrebleu-2.6.0-py3-none-any.whl.metadata (39 kB)\n",
                        "Collecting portalocker (from sacrebleu)\n",
                        "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
                        "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
                        "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
                        "Collecting colorama (from sacrebleu)\n",
                        "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
                        "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n",
                        "Downloading sacrebleu-2.6.0-py3-none-any.whl (100 kB)\n",
                        "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
                        "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
                        "Installing collected packages: portalocker, colorama, sacrebleu\n",
                        "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.6.0\n"
                    ]
                }
            ],
            "source": [
                "! pip install sacrebleu"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "EYX6ZAs-NAvl",
                "outputId": "efb1a6c3-5630-4c30-a0c5-098b1a286121"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Evaluation metrics loaded!\n"
                    ]
                }
            ],
            "source": [
                "# Load evaluation metrics\n",
                "rouge_metric = evaluate.load('rouge')\n",
                "bleu_metric = evaluate.load('sacrebleu')\n",
                "\n",
                "print(\"Evaluation metrics loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 101,
                    "referenced_widgets": [
                        "7df5e20f3a5640b9a72e19b02709c6d8",
                        "6a5eb7c9ea19475690440c6abee8a8d6",
                        "f28fd9a7691f4c37887c15cf1c7b18d8",
                        "256188eb4c95406d988571e3c9dfea9c",
                        "9ed388a9b57248d78a175739f2f58ea1",
                        "703053873e5547ad8eb50ea5f4692cc4",
                        "818ad8ede5bb4d009de071dc47c94e67",
                        "135d9fc4dbc14982a9893d0118ed3c94",
                        "513b41fc7b92443792c6c57012baad0a",
                        "12bbbd6a31a34e89951c5760af94b394",
                        "4164a7eb460f459591c967f0d4db4cfa"
                    ]
                },
                "id": "5AapRgyFNDKa",
                "outputId": "c74819fc-15cd-4b63-dfce-abfe77af04ef"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best model: Exp3_HigherRank\n",
                        "Eval Loss: 1.1761\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7df5e20f3a5640b9a72e19b02709c6d8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best model loaded for evaluation!\n"
                    ]
                }
            ],
            "source": [
                "# Select best model (lowest eval loss)\n",
                "best_experiment = experiment_table.loc[experiment_table['Final Eval Loss'].astype(float).idxmin()]\n",
                "best_model_path = f\"./models/{best_experiment['Experiment']}\"\n",
                "\n",
                "print(f\"Best model: {best_experiment['Experiment']}\")\n",
                "print(f\"Eval Loss: {best_experiment['Final Eval Loss']}\")\n",
                "\n",
                "# Load the best model for evaluation\n",
                "from peft import PeftModel\n",
                "\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config, # Use the already defined bnb_config\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "fine_tuned_model = PeftModel.from_pretrained(base_model, best_model_path)\n",
                "fine_tuned_model.eval()\n",
                "\n",
                "print(\"Best model loaded for evaluation!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "I1eOW_DmNGdy",
                "outputId": "e24c264f-efe2-4c0b-ba3d-605b2911f168"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test generation:\n",
                        "Instruction: Analyze the following math dialogue:\n",
                        "Generated Response: Professor A: Good afternoon, Professor B. I was reviewing some mathematical problems and came across a difficult one. The problem involves two people, John and Tom, who are both interested in solving ...\n"
                    ]
                }
            ],
            "source": [
                "# Generate predictions for evaluation\n",
                "def generate_response(model, tokenizer, instruction, max_length=200):\n",
                "    \"\"\"\n",
                "    Generate response from model given an instruction.\n",
                "    \"\"\"\n",
                "    prompt = f\"\"\"### Instruction:\n",
                "{instruction}\n",
                "\n",
                "### Response:\n",
                "\"\"\"\n",
                "\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
                "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
                "\n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_length,\n",
                "            temperature=0.7,\n",
                "            top_p=0.9,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "\n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    # Extract only the response part\n",
                "    response = response.split(\"### Response:\")[-1].strip()\n",
                "\n",
                "    return response\n",
                "\n",
                "# Test generation\n",
                "# Extract instruction from the 'text' field of the eval_dataset example\n",
                "example_text = eval_dataset[0]['text']\n",
                "# The instruction is everything between '### Instruction:' and '### Response:'\n",
                "parts = example_text.split(\"### Response:\", 1)\n",
                "instruction_part = parts[0]\n",
                "test_instruction = instruction_part.split(\"### Instruction:\\n\", 1)[1].strip()\n",
                "\n",
                "test_response = generate_response(fine_tuned_model, tokenizer, test_instruction)\n",
                "\n",
                "print(\"Test generation:\")\n",
                "print(f\"Instruction: {test_instruction}\")\n",
                "print(f\"Generated Response: {test_response[:200]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "EYhzdY9ANJm8",
                "outputId": "e23a29c9-9d19-4a01-d59a-fb932219dff6"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating predictions for 50 samples...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 40%|      | 20/50 [06:57<09:30, 19.01s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Skipping example 20 due to empty instruction or reference.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|| 50/50 [16:02<00:00, 19.25s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Predictions generated!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Calculate metrics on evaluation set (sample for efficiency)\n",
                "EVAL_SAMPLES = 50  # Adjust based on computational resources\n",
                "\n",
                "predictions = []\n",
                "references = []\n",
                "\n",
                "print(f\"Generating predictions for {EVAL_SAMPLES} samples...\")\n",
                "for i in tqdm(range(min(EVAL_SAMPLES, len(eval_dataset)))): # Note: len() works on materialized dataset\n",
                "    example = eval_dataset[i]\n",
                "\n",
                "    # Extract instruction and reference from the 'text' field\n",
                "    example_text = example['text']\n",
                "    parts = example_text.split(\"### Response:\", 1)\n",
                "\n",
                "    instruction_part = parts[0]\n",
                "    # Ensure instruction_part is not empty before splitting further\n",
                "    if \"### Instruction:\" in instruction_part:\n",
                "        instruction = instruction_part.split(\"### Instruction:\\n\", 1)[1].strip()\n",
                "    else:\n",
                "        # Fallback if instruction format is unexpected, or skip example\n",
                "        instruction = \"\"\n",
                "        # Optionally, you might want to log a warning or skip this example\n",
                "\n",
                "    # The reference is the part after \"### Response:\"\n",
                "    reference = parts[1].strip() if len(parts) > 1 else \"\"\n",
                "\n",
                "    # Ensure both instruction and reference are not empty\n",
                "    if not instruction or not reference:\n",
                "        tqdm.write(f\"Skipping example {i} due to empty instruction or reference.\")\n",
                "        continue\n",
                "\n",
                "    prediction = generate_response(fine_tuned_model, tokenizer, instruction)\n",
                "\n",
                "    predictions.append(prediction)\n",
                "    references.append(reference)\n",
                "\n",
                "print(\"\\nPredictions generated!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "vF8zg0baNMn_",
                "outputId": "74026ad6-fdcc-4b5d-c895-345cb8ae633a"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "ROUGE SCORES\n",
                        "==================================================\n",
                        "rouge1: 0.1963\n",
                        "rouge2: 0.0399\n",
                        "rougeL: 0.1182\n",
                        "rougeLsum: 0.1842\n"
                    ]
                }
            ],
            "source": [
                "# Calculate ROUGE scores\n",
                "rouge_results = rouge_metric.compute(\n",
                "    predictions=predictions,\n",
                "    references=references,\n",
                "    use_stemmer=True\n",
                ")\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ROUGE SCORES\")\n",
                "print(\"=\"*50)\n",
                "for key, value in rouge_results.items():\n",
                "    print(f\"{key}: {value:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Mh8zX9HZNPJ-",
                "outputId": "acc00759-bf0f-4343-8ef8-a64135fafb51"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "BLEU SCORE\n",
                        "==================================================\n",
                        "BLEU: 0.1746\n"
                    ]
                }
            ],
            "source": [
                "# Calculate BLEU score\n",
                "# sacrebleu expects references as list of lists\n",
                "bleu_results = bleu_metric.compute(\n",
                "    predictions=predictions,\n",
                "    references=[[ref] for ref in references]\n",
                ")\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"BLEU SCORE\")\n",
                "print(\"=\"*50)\n",
                "print(f\"BLEU: {bleu_results['score']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ItFuajzgNRo6",
                "outputId": "b7c54ac2-3da0-4887-8141-842746465a89"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Calculating perplexity: 100%|| 100/100 [00:14<00:00,  6.97it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "PERPLEXITY\n",
                        "==================================================\n",
                        "Perplexity: 3.5487\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Calculate Perplexity\n",
                "import math\n",
                "\n",
                "def calculate_perplexity(model, tokenized_dataset, num_samples=100):\n",
                "    \"\"\"\n",
                "    Calculate perplexity on a dataset.\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    total_tokens = 0\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for i in tqdm(range(min(num_samples, len(tokenized_dataset))), desc=\"Calculating perplexity\"):\n",
                "            example = tokenized_dataset[i]\n",
                "            input_ids = torch.tensor([example['input_ids']]).to(model.device)\n",
                "            labels = torch.tensor([example['labels']]).to(model.device)\n",
                "\n",
                "            outputs = model(input_ids, labels=labels)\n",
                "            loss = outputs.loss\n",
                "\n",
                "            total_loss += loss.item() * len(example['input_ids'])\n",
                "            total_tokens += len(example['input_ids'])\n",
                "\n",
                "    avg_loss = total_loss / total_tokens\n",
                "    perplexity = math.exp(avg_loss)\n",
                "\n",
                "    return perplexity\n",
                "\n",
                "perplexity = calculate_perplexity(fine_tuned_model, tokenized_eval)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"PERPLEXITY\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Perplexity: {perplexity:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "gt-3o9RvNT4P",
                "outputId": "0cd00835-235e-47b8-b9e9-757c0c6a2ce4"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "EVALUATION SUMMARY\n",
                        "==================================================\n",
                        "    Metric  Score\n",
                        "   ROUGE-1 0.1963\n",
                        "   ROUGE-2 0.0399\n",
                        "   ROUGE-L 0.1182\n",
                        "      BLEU 0.1746\n",
                        "Perplexity 3.5487\n",
                        "\n",
                        "Evaluation metrics saved to 'evaluation_metrics.csv'\n"
                    ]
                }
            ],
            "source": [
                "# Create evaluation summary\n",
                "evaluation_summary = pd.DataFrame([\n",
                "    {'Metric': 'ROUGE-1', 'Score': f\"{rouge_results['rouge1']:.4f}\"},\n",
                "    {'Metric': 'ROUGE-2', 'Score': f\"{rouge_results['rouge2']:.4f}\"},\n",
                "    {'Metric': 'ROUGE-L', 'Score': f\"{rouge_results['rougeL']:.4f}\"},\n",
                "    {'Metric': 'BLEU', 'Score': f\"{bleu_results['score']:.4f}\"},\n",
                "    {'Metric': 'Perplexity', 'Score': f\"{perplexity:.4f}\"}\n",
                "])\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"EVALUATION SUMMARY\")\n",
                "print(\"=\"*50)\n",
                "print(evaluation_summary.to_string(index=False))\n",
                "\n",
                "evaluation_summary.to_csv('evaluation_metrics.csv', index=False)\n",
                "print(\"\\nEvaluation metrics saved to 'evaluation_metrics.csv'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1sjNLStNNWn9"
            },
            "source": [
                "**7. Model Comparison: Base vs Fine-tuned**\n",
                "\n",
                "Compare responses from the base model and fine-tuned model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 66,
                    "referenced_widgets": [
                        "903ea79b1cdb43419607fc74adac4206",
                        "f1593ef3ac674da0a8a9a621327f1af5",
                        "e743416944a04cfe93592528e8fabaab",
                        "e06fd17f944c41af97dc8bf2b3c56b70",
                        "91eb46975b294c3191af29ecebddb73e",
                        "95de8651fca7481b9ee93d8890e80e04",
                        "def08a8c75f94b6dbdbffc40ee757fea",
                        "7d814dabda054301bd6ee98a9600e2bd",
                        "3715b760dd874f85b519daec9d7edf11",
                        "d3b492ae577644299f5093c1cc19a612",
                        "9d91ddb6dc904d39ba696c35ac8ba7cd"
                    ]
                },
                "id": "UNeWNEAlNa_1",
                "outputId": "c0c02a3c-d1d9-4bb0-e86d-a0e02e99b81a"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "903ea79b1cdb43419607fc74adac4206",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Base model loaded for comparison!\n"
                    ]
                }
            ],
            "source": [
                "# Load base model for comparison\n",
                "base_model_for_comparison = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config, # Use the already defined bnb_config\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "base_model_for_comparison.eval()\n",
                "\n",
                "print(\"Base model loaded for comparison!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "s4zce1TSNdQf",
                "outputId": "262516e7-4d67-4a27-efad-a58588cd73d1"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "====================================================================================================\n",
                        "BASE MODEL vs FINE-TUNED MODEL COMPARISON\n",
                        "====================================================================================================\n",
                        "\n",
                        "====================================================================================================\n",
                        "Question 1: What is the Pythagorean theorem?\n",
                        "====================================================================================================\n",
                        "\n",
                        "[BASE MODEL]\n",
                        "The Pythagorean theorem is a mathematical theorem that relates the sides of a right-angled triangle to each other, and the length of the perpendicular between them. The theorem states that the square of the hypotenuse (side opposite the right angle) is equal to the sum of the squares of the other two sides, which is equal to the square of the longer side multiplied by a constant known as the hypotenuse ratio. This ratio is called the hypotenuse ratio or the length of the perpendicular.\n",
                        "\n",
                        "[FINE-TUNED MODEL]\n",
                        "The Pythagorean theorem is a theorem in geometry that states that in a right-angled triangle, the square of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides. This theorem is named after the ancient Greek mathematician Pythagoras.\n",
                        "\n",
                        "The theorem is expressed as follows:\n",
                        "$$\n",
                        "\\text{Hypotenuse}^2 = \\text{Square of hypotenuse} + \\text{Square of opposite side}\n",
                        "$$\n",
                        "\n",
                        "This theorem is a key concept in trigonometry, as it relates the lengths of sides of a right-angled triangle to the square of the hypotenuse.\n",
                        "\n",
                        "\n",
                        "====================================================================================================\n",
                        "Question 2: How do you solve a quadratic equation?\n",
                        "====================================================================================================\n",
                        "\n",
                        "[BASE MODEL]\n",
                        "To solve a quadratic equation, we need to find the values of the variables and their squares, and then find the values of the quadratic formula. The quadratic formula is a formula that finds the values of the variables in a quadratic equation. To solve a quadratic equation using the quadratic formula, we first find the values of the variables in the equation:\n",
                        "\n",
                        "- `a` = 2\n",
                        "- `b` = 3\n",
                        "- `c` = 1\n",
                        "\n",
                        "Then, we find the values of the quadratic formula:\n",
                        "\n",
                        "- `x1` = (-b  (b - 4ac)) / 2a\n",
                        "- `x2` = (-b  (b -\n",
                        "\n",
                        "[FINE-TUNED MODEL]\n",
                        "To solve a quadratic equation, you need to identify the degree of the equation and then use the quadratic formula. Here's how to solve it:\n",
                        "\n",
                        "1. Identify the degree:\n",
                        "   - The degree of a quadratic equation is equal to the number of terms in the equation. In our equation, the degree is 3.\n",
                        "   - The equation $x^2 + 4x + 2 = 0$ has the same degree.\n",
                        "\n",
                        "2. Use the quadratic formula:\n",
                        "   - The quadratic formula is a method for solving quadratic equations. It involves taking the square root of the coefficients and multiplying by the corresponding exponents.\n",
                        "   - For our equation:\n",
                        "   $$\n",
                        "   \\begin{\n",
                        "\n",
                        "\n",
                        "====================================================================================================\n",
                        "Question 3: Explain the concept of derivatives in calculus.\n",
                        "====================================================================================================\n",
                        "\n",
                        "[BASE MODEL]\n",
                        "Derivatives are a fundamental concept in calculus, and it is used to study the change or variation of a function over time. In simple terms, a derivative tells us how much the function changes when we change a parameter, or change the value of a variable. In calculus, we study derivatives of functions, which can be used to find the slope of a line, find the area under a curve, or to approximate the value of a function at certain points.\n",
                        "\n",
                        "Derivatives can be used to find the slope of a line by taking the derivative of a function at a given point. Let's say we have the function:\n",
                        "\n",
                        "y = x ** 3\n",
                        "\n",
                        "To find the slope of the line y = x,\n",
                        "\n",
                        "[FINE-TUNED MODEL]\n",
                        "Calculus is the branch of mathematics that studies the behavior of functions. A function is a set of values, and its derivative is a number that represents how the function changes as you move away from a given point. This process is called differentiation.\n",
                        "\n",
                        "For example, if the function $ f(x) = x^2 $ is differentiable at the point $ x = 2 $, then its derivative $ f'(2) $ is the number $ 2 $ itself. This derivative tells us how the function changes as you move away from the point $ x = 2 $.\n",
                        "\n",
                        "The derivative is a number that represents how the function changes as you move away from a given point. The derivative tells us how much the function changes\n",
                        "\n",
                        "\n",
                        "====================================================================================================\n",
                        "Question 4: What is the difference between mean and median?\n",
                        "====================================================================================================\n",
                        "\n",
                        "[BASE MODEL]\n",
                        "Mean and median are two different mathematical concepts. The mean is the average of a set of data, while the median is the middle value in a set of data. The median is often used in statistics to determine the center of a data set. It is the middle value of the data, which is considered the \"average\" of the data. The mean, on the other hand, is the total sum of the data divided by the number of data points.\n",
                        "\n",
                        "[FINE-TUNED MODEL]\n",
                        "The mean (or average) is the sum of all the numbers divided by the number of numbers. It is calculated as follows:\n",
                        "\n",
                        "$$\n",
                        "\\text{Mean} = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
                        "$$\n",
                        "\n",
                        "The median, on the other hand, is the middle number in a list. It is calculated as follows:\n",
                        "\n",
                        "$$\n",
                        "\\text{Median} = \\frac{1}{n} \\left(x_1 + x_2 + \\ldots + x_n \\right)\n",
                        "$$\n",
                        "\n",
                        "Therefore, the difference between the mean and median is the sum of the difference between the mean and the median, which is equal to\n",
                        "\n",
                        "\n",
                        "====================================================================================================\n",
                        "Question 5: Solve for x: 2x + 5 = 15\n",
                        "====================================================================================================\n",
                        "\n",
                        "[BASE MODEL]\n",
                        "x = -5/2\n",
                        "\n",
                        "### Code:\n",
                        "```\n",
                        "import java.util.Scanner;\n",
                        "\n",
                        "public class Main {\n",
                        "    public static void main(String[] args) {\n",
                        "        Scanner scanner = new Scanner(System.in);\n",
                        "\n",
                        "        System.out.print(\"Enter a number: \");\n",
                        "        double num = scanner.nextDouble();\n",
                        "\n",
                        "        System.out.println(\"The square root of \" + num + \" is: \" + calculateSquareRoot(num));\n",
                        "\n",
                        "        scanner.close();\n",
                        "    }\n",
                        "\n",
                        "    public static double calculateSquareRoot(double num) {\n",
                        "        if (num == 0) {\n",
                        "            return 0;\n",
                        "\n",
                        "[FINE-TUNED MODEL]\n",
                        "Solver: To solve for x, we use the substitution method. Let's start by assuming that x is equal to 5. Then, we substitute 5 for x in the expression:\n",
                        "\n",
                        "\\[ 2x + 5 = 15 \\]\n",
                        "\n",
                        "Substitution:\n",
                        "\\[ 2x + 5 = 15 - 5 \\]\n",
                        "\n",
                        "Now, we can simplify the expression:\n",
                        "\n",
                        "\\[ 2x = 10 \\]\n",
                        "\n",
                        "Solving for x:\n",
                        "\\[ x = 5 \\]\n",
                        "\n",
                        "So, the solution to the problem is:\n",
                        "\n",
                        "\\[ x = 5 \\]\n",
                        "\n",
                        "This is the correct answer.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Test on sample math questions\n",
                "test_questions = [\n",
                "    \"What is the Pythagorean theorem?\",\n",
                "    \"How do you solve a quadratic equation?\",\n",
                "    \"Explain the concept of derivatives in calculus.\",\n",
                "    \"What is the difference between mean and median?\",\n",
                "    \"Solve for x: 2x + 5 = 15\"\n",
                "]\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"BASE MODEL vs FINE-TUNED MODEL COMPARISON\")\n",
                "print(\"=\"*100)\n",
                "\n",
                "for i, question in enumerate(test_questions, 1):\n",
                "    print(f\"\\n{'='*100}\")\n",
                "    print(f\"Question {i}: {question}\")\n",
                "    print(f\"{'='*100}\")\n",
                "\n",
                "    # Base model response\n",
                "    base_response = generate_response(base_model_for_comparison, tokenizer, question, max_length=150)\n",
                "    print(f\"\\n[BASE MODEL]\\n{base_response}\")\n",
                "\n",
                "    # Fine-tuned model response\n",
                "    finetuned_response = generate_response(fine_tuned_model, tokenizer, question, max_length=150)\n",
                "    print(f\"\\n[FINE-TUNED MODEL]\\n{finetuned_response}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "CBLwGlApNffn",
                "outputId": "0c174c82-e380-40ce-c1ec-2647841a023a"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "====================================================================================================\n",
                        "OUT-OF-DOMAIN QUERY HANDLING\n",
                        "====================================================================================================\n",
                        "\n",
                        "====================================================================================================\n",
                        "Question 1: What's the weather like today?\n",
                        "====================================================================================================\n",
                        "\n",
                        "[RESPONSE]\n",
                        "The weather is going to be very nice, with temperatures ranging from 80F to 85F. The humidity is 56%, which means the air is very moist. The wind is light, with a speed of 10 mph. The sky is clear, and the clouds are white. The forecast is for a mostly sunny day.\n",
                        "\n",
                        "I hope that helps. If you have any more questions, feel free to ask\n",
                        "\n",
                        "====================================================================================================\n",
                        "Question 2: Tell me a joke.\n",
                        "====================================================================================================\n",
                        "\n",
                        "[RESPONSE]\n",
                        "Alice: What do you call a pink elephant with a pink nose and a pink tongue?\n",
                        "Bob: An elephant with a pink nose and a pink tongue.\n",
                        "Alice: Oh, I see. So, what's the difference between a pink elephant and a pink panda?\n",
                        "Bob: A pink panda has a pink nose and a pink tongue, while a pink elephant has a\n",
                        "\n",
                        "====================================================================================================\n",
                        "Question 3: What is the capital of France?\n",
                        "====================================================================================================\n",
                        "\n",
                        "[RESPONSE]\n",
                        "The capital of France is Paris, which is the most populous city in France and the capital of the le-de-France region.\n",
                        "\n",
                        "### Notes:\n",
                        "- \"Paris\" is the capital of France, not just the city of Paris.\n",
                        "- The capital of the Ile-de-France region is also the capital of the le-de-France region, which includes Paris.\n",
                        "- The le-de-France region covers several other cities and\n"
                    ]
                }
            ],
            "source": [
                "# Qualitative analysis on out-of-domain queries\n",
                "out_of_domain_questions = [\n",
                "    \"What's the weather like today?\",\n",
                "    \"Tell me a joke.\",\n",
                "    \"What is the capital of France?\"\n",
                "]\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"OUT-OF-DOMAIN QUERY HANDLING\")\n",
                "print(\"=\"*100)\n",
                "\n",
                "for i, question in enumerate(out_of_domain_questions, 1):\n",
                "    print(f\"\\n{'='*100}\")\n",
                "    print(f\"Question {i}: {question}\")\n",
                "    print(f\"{'='*100}\")\n",
                "\n",
                "    response = generate_response(fine_tuned_model, tokenizer, question, max_length=100)\n",
                "    print(f\"\\n[RESPONSE]\\n{response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "FEvgcF8gNjQq"
            },
            "source": [
                "**8. Deployment with Gradio**\n",
                "\n",
                "Create an interactive web interface for the Math Education Assistant."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "WU1lgUuJNoCI",
                "outputId": "93961246-1b58-4682-af4d-cd92500bbd25"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model prepared for deployment!\n"
                    ]
                }
            ],
            "source": [
                "# Prepare model for deployment\n",
                "# Merge LoRA weights with base model for faster inference\n",
                "final_model = fine_tuned_model.merge_and_unload()\n",
                "final_model.eval()\n",
                "\n",
                "print(\"Model prepared for deployment!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 445
                },
                "id": "JbpEGZvYNqKb",
                "outputId": "bb935186-9dd6-4b01-c848-ca6f2e5233aa"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Launching Math Education Assistant...\n",
                        "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
                        "* Running on public URL: https://57b54929f569825073.gradio.live\n",
                        "\n",
                        "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-2040295417.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Launch the interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLaunching Math Education Assistant...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[0m\n\u001b[1;32m   2875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2876\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2877\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnetworking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2878\u001b[0m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2879\u001b[0m                     artifact = HTML(\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/networking.py\u001b[0m in \u001b[0;36murl_ok\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             if (\n\u001b[1;32m     70\u001b[0m                 \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m302\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m303\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m307\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, params, headers, cookies, auth, proxy, follow_redirects, verify, timeout, trust_env)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mon\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mHEAD\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mrequests\u001b[0m \u001b[0mshould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m     return request(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, params, content, data, files, json, headers, cookies, auth, proxy, timeout, follow_redirects, verify, trust_env)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mtrust_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     ) as client:\n\u001b[0;32m--> 109\u001b[0;31m         return client.request(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         )\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Define chatbot function for Gradio\n",
                "def math_tutor_chatbot(message, history):\n",
                "    \"\"\"\n",
                "    Chatbot function for Gradio interface.\n",
                "\n",
                "    Args:\n",
                "        message: User's current message\n",
                "        history: List of previous [user_msg, assistant_msg] pairs\n",
                "\n",
                "    Returns:\n",
                "        Generated response\n",
                "    \"\"\"\n",
                "    # Generate response\n",
                "    response = generate_response(final_model, tokenizer, message, max_length=300)\n",
                "    return response\n",
                "\n",
                "# Create Gradio interface\n",
                "demo = gr.ChatInterface(\n",
                "    fn=math_tutor_chatbot,\n",
                "    title=\"Math Education Assistant\",\n",
                "    description=\"\"\"Ask me any math question! I'm fine-tuned on mathematical problems and can help with:\n",
                "    - Algebra and equations\n",
                "    - Geometry and trigonometry\n",
                "    - Calculus concepts\n",
                "    - Statistics and probability\n",
                "    - Problem-solving strategies\n",
                "\n",
                "    **Note**: This is an AI assistant - always verify important calculations!\"\"\",\n",
                "    examples=[\n",
                "        \"What is the quadratic formula?\",\n",
                "        \"How do I calculate the area of a circle?\",\n",
                "        \"Explain what a derivative is.\",\n",
                "        \"Solve: 3x - 7 = 11\",\n",
                "        \"What's the difference between permutations and combinations?\"\n",
                "    ],\n",
                "    theme=\"soft\"\n",
                "    # Removed unsupported keyword arguments: retry_btn=\"Retry\", undo_btn=\"Undo\", clear_btn=\"Clear\"\n",
                ")\n",
                "\n",
                "# Launch the interface\n",
                "print(\"\\nLaunching Math Education Assistant...\")\n",
                "demo.launch(share=True, debug=True)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "authorship_tag": "ABX9TyMHnn1EqdPZCDCuw311oUmw",
            "gpuType": "T4",
            "include_colab_link": true,
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
